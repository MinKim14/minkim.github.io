<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Parallel - Code Blog</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <header>
        <nav>
            <div class="logo">Code Blog</div>
            <ul>
                <li><a href="../index.html">Home</a></li>
<li><a href="../web-development.html" class="">Web-Development</a></li>
<li><a href="../algorithms.html" class="">Algorithms</a></li>
<li><a href="../projects.html" class="">Projects</a></li>
<li><a href="../about.html" class="">About</a></li>
            </ul>
        </nav>
    </header>

    <main>
        <div class="color-bar top"></div>
        <section class="section-header">
            <h1>Parallel</h1>
            <p>Explore our collection of parallel articles and tutorials</p>
        </section>

        <section class="content-section">
            
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Gpu - Code Blog</title>
        <link rel="stylesheet" href="../styles.css">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/styles/default.min.css">
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/highlight.min.js"></script>
        <script>hljs.highlightAll();</script>
    </head>
    <body>
        <header>
            <nav>
                <div class="logo">Code Blog</div>
                <ul>
                    <li><a href="../index.html">Home</a></li>
                    <li><a href="../web-development.html">Web Development</a></li>
                    <li><a href="../algorithms.html">Algorithms</a></li>
                    <li><a href="../projects.html">Projects</a></li>
                    <li><a href="../about.html">About</a></li>
                </ul>
            </nav>
        </header>

        <main>
            <div class="color-bar top"></div>
            <article class="blog-post">
                <div class="post-header">
                    <h2 class="post-title">Gpu</h2>
                    <div class="post-meta">
                        <span class="post-date">April 17, 2025</span>
                        <span class="post-category">Parallel</span>
                    </div>
                </div>
                <div class="post-content">
                    <p class="post-paragraph">
\usepackage[utf8]{inputenc}</p>
<p class="post-paragraph"></p>
<p class="post-paragraph"></p>
<p class="post-paragraph"></p>
<p class="post-paragraph"></p>
<p class="post-paragraph">
 Utility}</p>
<p class="post-paragraph"></p>
<p class="post-paragraph">{rgb}{0.95,0.95,0.95}
\lstset{
  backgroundcolor=\color{codegray},
  basicstyle=\ttfamily\footnotesize,
  breaklines=true,
  frame=single,
  captionpos=b,
  tabsize=2,
  showstringspaces=false,
  commentstyle=\color{gray},
  keywordstyle=\color{blue},
  stringstyle=\color{red},
  language=Python
}</p>
<p class="post-paragraph"></p>
<p class="post-paragraph"></p>
<p class="post-paragraph"><h2 class="post-section">Overview</h2></p>
<p class="post-paragraph">When running deep learning models, it's important to parallelize Python functions across multiple GPUs—especially when dealing with large datasets or compute-heavy tasks. Today, I came across the <strong class="post-bold">Dynomo</strong> framework, and I was impressed by how cleanly it handles GPU parallelism. In this post, I walk through how Dynomo's <code class="post-code-inline">gpu\_map</code> function works under the hood and how you can use it to scale your multi-GPU workflows.</p>
<p class="post-paragraph"><h2 class="post-section">What is <code class="post-code-inline">gpu\_map</h2>?</code></p>
<p class="post-paragraph">The <code class="post-code-inline">gpu\_map</code> function is designed to distribute a Python function across multiple GPUs using <code class="post-code-inline">multiprocessing</code>. It supports two scheduling modes:</p>
<p class="post-paragraph"><ul class="post-list">
  <li class="post-list-item"><strong class="post-bold">Static</strong>: Evenly splits inputs across GPUs. Best when task runtimes are uniform.
  </li><li class="post-list-item"><strong class="post-bold">Dynamic</strong>: Assigns GPUs as they become available. Best for imbalanced workloads.
</ul></p>
<p class="post-paragraph"><h2 class="post-section">Function Signature</h2></p>
<p class="post-paragraph"><pre class="post-code-block language-python"><code>def gpu_map(func, args, n_ranks=None, gpus=None, method="static", progress_msg=None):</code></pre></p>
<p class="post-paragraph"><h3 class="post-subsection">Parameters</h3></p>
<p class="post-paragraph"><ul class="post-list">
  </li><li class="post-list-item"><code class="post-code-inline">func</code>: The function to be executed in parallel.
  </li><li class="post-list-item"><code class="post-code-inline">args</code>: A list of arguments to be passed to <code class="post-code-inline">func</code>.
  </li><li class="post-list-item"><code class="post-code-inline">n\_ranks</code>: Number of parallel processes (default: number of GPUs).
  </li><li class="post-list-item"><code class="post-code-inline">gpus</code>: List of GPU device IDs (optional).
  </li><li class="post-list-item"><code class="post-code-inline">method</code>: <code class="post-code-inline">"static"</code> or <code class="post-code-inline">"dynamic"</code> scheduling.
  </li><li class="post-list-item"><code class="post-code-inline">progress\_msg</code>: Optional message to show a progress bar with <code class="post-code-inline">tqdm</code>.
</ul></p>
<p class="post-paragraph"><h2 class="post-section">Step-by-Step: Static Scheduling Mode</h2></p>
<p class="post-paragraph"><h3 class="post-subsection">1. Device Setup</h3>
If no GPU list is provided, it automatically detects available GPUs using the environment or <code class="post-code-inline">nvidia-smi</code>.</p>
<p class="post-paragraph"><h3 class="post-subsection">2. Static Argument Distribution</h3></p>
<p class="post-paragraph"><pre class="post-code-block language-python"><code>args_by_rank = [args[rank::n_ranks] for rank in range(n_ranks)]
args_by_rank = [[a + [gpus[i]] for a in args_by_rank[i]] for i in range(n_ranks)]</code></pre></p>
<p class="post-paragraph">Each GPU gets every <code class="post-code-inline">n\_ranks</code>-th item from <code class="post-code-inline">args</code>, and each argument is extended with the GPU ID.</p>
<p class="post-paragraph"><h3 class="post-subsection">3. Process Spawning and Execution</h3></p>
<p class="post-paragraph"><pre class="post-code-block language-python"><code>proc_args = (func, args_by_rank[rank], rank, result_queue, progress_msg)
proc = mp.Process(target=gpu_map_static_helper, args=proc_args)</code></pre></p>
<p class="post-paragraph">Each subprocess runs a helper function that executes its chunk of work.</p>
<p class="post-paragraph"><h3 class="post-subsection">4. Result Aggregation</h3></p>
<p class="post-paragraph"><pre class="post-code-block language-python"><code>outputs = []
for it in range(len(args)):
    rank = it % n_ranks
    idx = it // n_ranks
    outputs.append(outputs_by_rank[rank][idx])</code></pre></p>
<p class="post-paragraph">This reorders the results back into the original input order.</p>
<p class="post-paragraph"><h2 class="post-section">Dynamic Scheduling Mode</h2></p>
<p class="post-paragraph">Instead of dividing the work in advance, tasks are launched one-by-one as GPUs become available. A queue tracks available GPUs, and tasks are dynamically assigned.</p>
<p class="post-paragraph"><pre class="post-code-block language-python"><code>gpu_queue = mp.Queue()
for rank in range(n_ranks):
    gpu_queue.put(gpus[rank % len(gpus)])</code></pre></p>
<p class="post-paragraph">Each task waits for a GPU, launches, and releases the GPU back to the queue upon completion.</p>
<p class="post-paragraph"><h2 class="post-section">Fallback Mode</h2></p>
<p class="post-paragraph">If <code class="post-code-inline">method=None</code>, the function executes serially:</p>
<p class="post-paragraph"><pre class="post-code-block language-python"><code>return [func(*arg) for arg in args]</code></pre></p>
<p class="post-paragraph"><h2 class="post-section">Helper Function: <code class="post-code-inline">gpu\_map\_static\_helper</h2></code></p>
<p class="post-paragraph"><pre class="post-code-block language-python"><code>def gpu_map_static_helper(func, args, rank, result_queue, progress_msg):
    if progress_msg is not None:
        args = tqdm.tqdm(args, desc=progress_msg)
    out = [func(*arg) if isinstance(arg, tuple) else func(arg) for arg in args]
    result_queue.put((rank, out))</code></pre></p>
<p class="post-paragraph"><h3 class="post-subsection">Line-by-Line Breakdown</h3>
<ul class="post-list">
  </li><li class="post-list-item">If a progress message is given, wraps the input with <code class="post-code-inline">tqdm</code> for a visual progress bar.
  </li><li class="post-list-item">Iterates over the input arguments, applying <code class="post-code-inline">func</code> to each.
  </li><li class="post-list-item">Automatically unpacks tuple inputs when needed.
  </li>\item Sends results back to the main process using a shared <code class="post-code-inline">Queue</code>, along with the process rank.
</ul></p>
<p class="post-paragraph"><h2 class="post-section">Example Usage</h2></p>
<p class="post-paragraph"><pre class="post-code-block language-python"><code>def render_scene(scene_data, camera_settings, gpu_id):
    torch.cuda.set_device(gpu_id)
    return render(scene_data, camera_settings)</p>
<p class="post-paragraph">input_args = [(scene, cam) for scene, cam in zip(scene_list, camera_list)]
results = gpu_map(render_scene, input_args, method="static", progress_msg="Rendering Scenes")</code></pre></p>
<p class="post-paragraph"><h2 class="post-section">Conclusion</h2></p>
<p class="post-paragraph">Dynomo's <code class="post-code-inline">gpu\_map</code> utility provides a clean and powerful abstraction for parallelizing workloads over multiple GPUs. Whether you're training, rendering, or processing in parallel, this tool allows you to scale seamlessly with minimal code changes. It supports static and dynamic scheduling, manages GPU visibility cleanly, and works well with CUDA-based workflows like PyTorch.</p>
<p class="post-paragraph"></p>
                </div>
                <div class="post-navigation">
                    <a href="../parallel.html" class="back-button">← Back to Parallel</a>
                    <a href="../index.html" class="home-button">Home</a>
                </div>
            </article>
            <div class="color-bar bottom"></div>
        </main>

        <footer>
            <p>&copy; 2024 Code Blog. All rights reserved.</p>
        </footer>
    </body>
    </html>
    
        </section>
        <div class="color-bar bottom"></div>
    </main>

    <footer>
        <p>&copy; 2024 Code Blog. All rights reserved.</p>
    </footer>
</body>
</html>